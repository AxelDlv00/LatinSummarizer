{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Code for inference**\n",
    "\n",
    "This notebook focuses on two main tasks:\n",
    "\n",
    "1. **Inference for mT5 on English → Latin Translation**: Using a trained mT5 model to generate Latin translations from English inputs.\n",
    "2. **Inference on Summaries with Mistral**: Generating extractive summaries using the Mistral-7B-Instruct model.\n",
    "\n",
    "---\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "### 1. Inference for mT5 on Translation ('en' → 'la')\n",
    "- 1.1 **Load Libraries**\n",
    "- 1.2 **Define Global Parameters**\n",
    "- 1.3 **Load the Trained Tokenizer and Model**\n",
    "- 1.4 **Generate Translations**\n",
    "\n",
    "### 2. Inference on Summaries with Mistral ('en' → 'en')\n",
    "- 2.1 **Load Mistral Model for Summarization**\n",
    "- 2.2 **Generate Summaries with Mistral**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference for mT5 on translation 'en' -> 'la' along with the extraction of summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from utils.mT5_train import training_loop, plot_training\n",
    "from utils.generate_translation import generate_translation, generate_translation_with_options, inference_from_csv, inference_from_csv_adding_column\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from utils.bleu import calculate_bleu_and_chrf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_train = \"/Data/AxelDlv/LatinSummarizer/prompt_no_stanza_train.csv\"\n",
    "# path_to_test = \"/Data/AxelDlv/LatinSummarizer/prompt_no_stanza_test.csv\"\n",
    "# path_to_special_tokens = \"/Data/AxelDlv/LatinSummarizer/common_tags_la_en.csv\"  \n",
    "\n",
    "path_to_train = \"/Data/AxelDlv/LatinSummarizer/prompt_with_stanza_train.csv\"\n",
    "path_to_test = \"/Data/AxelDlv/LatinSummarizer/prompt_with_stanza_test.csv\"\n",
    "path_to_special_tokens = \"/Data/AxelDlv/LatinSummarizer/stanza_merged_tags_la_en.csv\"\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(path_to_train)\n",
    "test_data = pd.read_csv(path_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<la> <no_stanza> rogabis eum et exaudiet te et vota tua reddes <la> <la.en> <en> <no_stanza> \n",
      "Thou shalt pray to him, and he will hear thee, and thou shalt pay vows. <en>\n",
      "\n",
      "<en> <no_stanza> Here I ask, if sufficient protection is afforded to Hiempsal by the treaty  and if the Recentoric district is private property, what was use of excepting these lands by  name in the law? If that treaty itself has some obscurity in it, and if the Recentoric is  sometimes said to be public property, who do you suppose will believe that there have been two  interests found in the world, and only two, which he spared for nothing? Does there appear to  have been any coin in the world so carefully hidden that the architects of this law have failed  to scent it out? They are draining the provinces, the free cities, our allies, our friends, and  even the kings who are confederate with us. They are laying bands on the revenue of the Roman  people. <en> <en.la> <la> <no_stanza> \n",
      "hic quaero, si Hiempsali satis est cautum  foedere et Recentoricus ager privatus est, quid attinuerit excipi; sin et foedus  illud habet aliquam dubitationem et ager Recentoricus dicitur non numquam esse  publicus, quem putet existimaturum duas causas in orbe terrarum repertas quibus  gratis parceret. num quisnam tam abstrusus usquam nummus videtur quem  non architecti huiusce legis olfecerint? provincias , civitates  liberas, socios, amicos, reges denique exhauriunt, admovent manus vectigalibus  populi Romani. <la>\n",
      "\n",
      "<en> <no_stanza> Africans and warlike Spaniards fell alike, <en> <en.la> <la> <no_stanza> \n",
      "ast hic, tranquillo qua labitur agmine flumen, ducit corticeis fluitantia retia signis; <la>\n",
      "\n",
      "<en> <no_stanza> For who does not realize that if Caesar had not raised an army, Antonius return would have entailed our destruction? <en> <en.la> <la> <no_stanza> \n",
      "Quis est enim qui hoc non intellegat, nisi Caesar exercitum paravisset, non sine exitio nostro futurum Antoni reditum fuisse? <la>\n",
      "\n",
      "<la> 9 Sed dicebat, quod aliquis dicitur videre non solum in re illuminata per solem, sed etiam in sole ipso, inquantum per illuminationem solis videt.- Sed contra est quod in sole non resultant rerum similitudines visibilium, quod videtur ad rationem speculi pertinere. Ergo videtur quod videre in sole non sic dicatur sicut videre in speculo. arg. 10 Praeterea, visio qua videtur Deus ut est obiectum beatitudinis, est dignior quam illa qua videtur ut species rerum: quia illa facit beatum, ista vero non. Sed ad videndum Deum ut est beatitudinis obiectum, homo in statu viae existens potest elevari maiori elevatione, qua scilicet mens abstrahitur omnino a sensibus, ut est in raptu. Ergo minori elevatione etiam sine raptu potest elevari mens prophetae ad videndum essentiam divinam, ut est species rerum; et sic propheta potest videre in speculo aeternitatis. arg. 11 Praeterea, plus distat essentia divina, prout in se consideratur, et prout est similitudo alterius rei, quam prout est similitudo unius rei et similitudo alterius: quia plus distat Deus a creatura quam una ab alia. Sed aliquis potest videre Deum ut est species unius creaturae, sine hoc quod videat eum ut est species creaturae alterius; alias oporteret quod omnes videntes Deum omnia cognoscerent. Ergo aliquis potest videre eum ut est species quarumdam rerum, sine hoc quod videat essentiam eius in seipsa. Ergo etiam qui non vident Deum per essentiam, possunt videre in speculo aeternitatis; quod maxime videtur prophetis competere. arg. 12 Praeterea, Augustinus dicit, VI de Trinitate, quod quorumdam mentes ita elevantur, ut in ipsa suprema rerum arce incommutabiles rationes inspiciant. Sed prophetarum mentes videntur esse maxime elevatae. Ergo videtur quod ea quae prophetice vident, in ipsa rerum arce inspiciant, scilicet divina essentia; et sic idem quod prius. arg. 13 Praeterea, iudicium non potest esse de aliquo nisi per id quod est eo superius, ut patet per Augustinum in Lib. de vera Relig. Sed ea de quibus prophetae iudicant, sunt rerum immobiles veritates. Ergo non potest esse quod de eis iudicent per aliquid temporale et mobile, sed per immobilem veritatem, quae est ipse Deus; et sic idem quod prius. s. c. 1 Sed contra. Est quod Luc. X, 24, super illud, dico vobis quod multi prophetae et reges etc. dicit Glossa: prophetae et iusti a longe gloriam Dei viderunt per speculum in aenigmate. Sed qui videt in ipsa aeterna praescientia Dei, non videt in aenigmate. <la> <la.la> <la>\n",
      "9 Sed dicebat, quod aliquis dicitur videre non solum in re illuminata per solem, sed etiam in sole ipso, inquantum per illuminationem solis videt.- Sed contra est quod in sole non resultant rerum similitudines visibilium, quod videtur ad rationem speculi pertinere. Ergo videtur quod videre in sole non sic dicatur sicut videre in speculo. Ergo videtur quod ea quae prophetice vident, in ipsa rerum arce inspiciant, scilicet divina essentia; et sic idem quod prius. Sed ea de quibus prophetae iudicant, sunt rerum immobiles veritates. Sed qui videt in ipsa aeterna praescientia Dei, non videt in aenigmate. <la>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>prefix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;la&gt; &lt;no_stanza&gt; rogabis eum et exaudiet te et...</td>\n",
       "      <td>Thou shalt pray to him, and he will hear thee,...</td>\n",
       "      <td>la.en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;en&gt; &lt;no_stanza&gt; Here I ask, if sufficient pro...</td>\n",
       "      <td>hic quaero, si Hiempsali satis est cautum  foe...</td>\n",
       "      <td>en.la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;en&gt; &lt;no_stanza&gt; Africans and warlike Spaniard...</td>\n",
       "      <td>ast hic, tranquillo qua labitur agmine flumen,...</td>\n",
       "      <td>en.la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;en&gt; &lt;no_stanza&gt; For who does not realize that...</td>\n",
       "      <td>Quis est enim qui hoc non intellegat, nisi Cae...</td>\n",
       "      <td>en.la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;la&gt; 9 Sed dicebat, quod aliquis dicitur vider...</td>\n",
       "      <td>9 Sed dicebat, quod aliquis dicitur videre non...</td>\n",
       "      <td>la.la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633514</th>\n",
       "      <td>&lt;la&gt; &lt;with_stanza&gt; Duo &lt;NUM&gt; tamen &lt;ADV&gt; agger...</td>\n",
       "      <td>However &lt;ADV&gt; , &lt;PUNCT&gt; two &lt;NUM&gt; lofty &lt;ADJ&gt; ...</td>\n",
       "      <td>la.en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633515</th>\n",
       "      <td>&lt;la&gt; O homo, audi et intellige verba illius qu...</td>\n",
       "      <td>O homo, audi et intellige verba illius qui era...</td>\n",
       "      <td>la.la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633516</th>\n",
       "      <td>&lt;en&gt; &lt;with_stanza&gt; And &lt;CCONJ&gt; he &lt;PRON&gt; shall...</td>\n",
       "      <td>et reget illas in virga ferrea tamquam vas fig...</td>\n",
       "      <td>en.la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633517</th>\n",
       "      <td>&lt;la&gt; ad 7 Ad septimum dicendum, quod duplicite...</td>\n",
       "      <td>ad 7 Ad septimum dicendum, quod dupliciter ali...</td>\n",
       "      <td>la.la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633518</th>\n",
       "      <td>&lt;la&gt; ad 14 Ad decimumquartum dicendum quod iud...</td>\n",
       "      <td>ad 14 Ad decimumquartum dicendum quod iudicium...</td>\n",
       "      <td>la.la</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>633519 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   prompt  \\\n",
       "0       <la> <no_stanza> rogabis eum et exaudiet te et...   \n",
       "1       <en> <no_stanza> Here I ask, if sufficient pro...   \n",
       "2       <en> <no_stanza> Africans and warlike Spaniard...   \n",
       "3       <en> <no_stanza> For who does not realize that...   \n",
       "4       <la> 9 Sed dicebat, quod aliquis dicitur vider...   \n",
       "...                                                   ...   \n",
       "633514  <la> <with_stanza> Duo <NUM> tamen <ADV> agger...   \n",
       "633515  <la> O homo, audi et intellige verba illius qu...   \n",
       "633516  <en> <with_stanza> And <CCONJ> he <PRON> shall...   \n",
       "633517  <la> ad 7 Ad septimum dicendum, quod duplicite...   \n",
       "633518  <la> ad 14 Ad decimumquartum dicendum quod iud...   \n",
       "\n",
       "                                                   answer prefix  \n",
       "0       Thou shalt pray to him, and he will hear thee,...  la.en  \n",
       "1       hic quaero, si Hiempsali satis est cautum  foe...  en.la  \n",
       "2       ast hic, tranquillo qua labitur agmine flumen,...  en.la  \n",
       "3       Quis est enim qui hoc non intellegat, nisi Cae...  en.la  \n",
       "4       9 Sed dicebat, quod aliquis dicitur videre non...  la.la  \n",
       "...                                                   ...    ...  \n",
       "633514  However <ADV> , <PUNCT> two <NUM> lofty <ADJ> ...  la.en  \n",
       "633515  O homo, audi et intellige verba illius qui era...  la.la  \n",
       "633516  et reget illas in virga ferrea tamquam vas fig...  en.la  \n",
       "633517  ad 7 Ad septimum dicendum, quod dupliciter ali...  la.la  \n",
       "633518  ad 14 Ad decimumquartum dicendum quod iudicium...  la.la  \n",
       "\n",
       "[633519 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print samples of coupleq prompt / answer \n",
    "for i in range(5):\n",
    "    print(train_data.iloc[i]['prompt'])\n",
    "    print(train_data.iloc[i]['answer'])\n",
    "    print()\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<la> <no_stanza> Aperuerat iam Italiam bellumque transmiserat, ut supra memoravimus, ala Siliana, nullo apud quemquam Othonis favore, nec quia Vitellium mallent, sed longa pax ad omne servitium fregerat facilis occupantibus et melioribus incuriosos. <la> <la.en> <en> <no_stanza> \n",
      "The road into Italy had already been opened and the war transferred there by Siliuss cavalry, as we have said above. Although no one favoured Otho there, this success was not due to the preference of the people for Vitellius; but long peace had broken their spirits, so that they were ready for any kind of servitude, an easy prey to the first comer and careless as to who had the better cause. <en>\n",
      "\n",
      "<la> <no_stanza> Usus practicus unus est ad altitudinem geopotentialis in meteorologia. <la> <la.en> <en> <no_stanza> \n",
      "One practical use is for altitude of geopotential heights in meteorology. <en>\n",
      "\n",
      "<la> <with_stanza> praeferunt <VERB> gustandi <VERB> discretionem <NOUN> : <PUNCT> tamquam <SCONJ> non <PART> plurimum <DET> hinc <ADV> nos <PRON> cedamus <VERB> et <CCONJ> simio <VERB> . <PUNCT> <la> <clue> <SCONJ> <SCONJ> <PRON> <AUX> <PART> <ADV> <ADJ> <ADP> <PRON> <ADV> <ADP> <DET> <NOUN> <PUNCT> <clue> <la.en> <en> <no_stanza> \n",
      "As if we were not far inferior in this even to the ape! <en>\n",
      "\n",
      "<en> <no_stanza> But let all things be done decently and according to order. <en> <en.la> <la> <no_stanza> \n",
      "omnia autem honeste et secundum ordinem fiant <la>\n",
      "\n",
      "<la> Affert secundus ramus persicum, ex persico, et nucipersico interuallis pluribus aeque distinctum, exossem, et ramulo nunc persicum, nunc nucipersicum protuberans, si aliquod pomum nucleum habebat, dulcem uti amygdala dabat, et nunc hominum, nunc animalium faciem ementiebatur, diuersaque demonstrabat lineamenta. Dat alter cerasia exossa, acida, dulciaque, dat mala aurea, cortex floribus, rosisque consita, fructus debitam omnem superantes magnitudinem, dulciores, odoratiores, uerno tempore efflorescentes, debitum ultra tempus fructus producentes: diutiusque immorabantur, continuae foecunditatis orbem perpetuum annum ministrabant, per quosdam gradus poma sibi succedebant, foetura interpollabatur, brachia ponderibus inclinata procumbebant, et denique sic coelum fauebat, ut pulchriorem non conspexerim. De huiusmodi re iam satis: longius enim euagati sumus, quam par fuerat, quorum aliqua ex maiorum monumentis excerpsimus, tempori, regionique accommodantes, quaeque multis erant experimentis cognita, multis praeterea ingeniosis, et utillimis auximus. CAPUT IX. Diuersa ignaria parandi ratio. FORTUITO Euenisse narrat Uictruuius, quod diuersae arbores, crebraeque, uentis, et tempestatibus exagitatae attritu ualido ramos terentes, collisis inter se partibus, et rarefactis calorem euocarint, et ignem excitarint, unde magna successit flamma. efferati, et agrestes homines inde perterriti terga fugae dedere, sic iam sopita flamma, propius cum accessissent, et magnam inde humanis corporibus perpendissent accedere commoditatem, ignem conseruabant: sic urbanitatis, consuetudinis, et sermonis causas praebuisse, inde pastorum in solitudinibus, et militum in castris explorauit necessitas, diuersa quomodo parari ignaria possint, cum semper abstrusum ignem ad excutiendum, filicis occasio non detur, et docuerunt, quae nam ligna essent, usui accommodata: et quanquam terebrum, et conceptaculum, ex eodem fieri nonnumquam eueniat, tamen ex uno, quod agat, atque efficiendi uim retineat, ex altero uer, quod patiatur, ex molli uno, forti altero faciundum censent. Exemplo Ligna, quae attritu ignem concipiunt, Sunt, quae maxim calida, uti laurus, rhamnus, ilex, et tilia. Menestor uero morum addit, et coniiciunt, quod ascias illic retundent, et hebetent; ex his omnibus terebrum faciunt, ut attritu acrius resistat, et pertinacius opus expediat, conceptaculum uer ex molli: uti hedera, ferula, syluestri uite, et similibus arefactis, et penitus humore omni ex inanitis: ad summum ea deteriora ad ignarium usum ueniunt, et reiiciuntur, quae umbrosis, contectisque locis assurgunt. Et sic commodius expedies: nec referre arbitror, si laurum lauro, uel hedera enudata, ferulam ferula confricaueris, et quod praestantius est fune super eam celeriter, et uehementer moto, et ubi fumum eleuabit, adiecto minuti sulphuris momento, fomitem admoueris, uel arida nutrimenta, quae ex materia parabis aridi fungi: foliorum excocti uelleris, circa tussilaginis radices reperti: ocyus enim ignem concipiunt, retinentque. <la> <la.la> <la>\n",
      "Affert secundus ramus persicum, ex persico, et nucipersico interuallis pluribus aeque distinctum, exossem, et ramulo nunc persicum, nunc nucipersicum protuberans, si aliquod pomum nucleum habebat, dulcem uti amygdala dabat, et nunc hominum, nunc animalium faciem ementiebatur, diuersaque demonstrabat lineamenta. Diuersa ignaria parandi ratio. FORTUITO Euenisse narrat Uictruuius, quod diuersae arbores, crebraeque, uentis, et tempestatibus exagitatae attritu ualido ramos terentes, collisis inter se partibus, et rarefactis calorem euocarint, et ignem excitarint, unde magna successit flamma. Menestor uero morum addit, et coniiciunt, quod ascias illic retundent, et hebetent; ex his omnibus terebrum faciunt, ut attritu acrius resistat, et pertinacius opus expediat, conceptaculum uer ex molli: uti hedera, ferula, syluestri uite, et similibus arefactis, et penitus humore omni ex inanitis: ad summum ea deteriora ad ignarium usum ueniunt, et reiiciuntur, quae umbrosis, contectisque locis assurgunt. Et sic commodius expedies: nec referre arbitror, si laurum lauro, uel hedera enudata, ferulam ferula confricaueris, et quod praestantius est fune super eam celeriter, et uehementer moto, et ubi fumum eleuabit, adiecto minuti sulphuris momento, fomitem admoueris, uel arida nutrimenta, quae ex materia parabis aridi fungi: foliorum excocti uelleris, circa tussilaginis radices reperti: ocyus enim ignem concipiunt, retinentque. <la>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>prefix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;la&gt; &lt;no_stanza&gt; Aperuerat iam Italiam bellumq...</td>\n",
       "      <td>The road into Italy had already been opened an...</td>\n",
       "      <td>la.en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;la&gt; &lt;no_stanza&gt; Usus practicus unus est ad al...</td>\n",
       "      <td>One practical use is for altitude of geopotent...</td>\n",
       "      <td>la.en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;la&gt; &lt;with_stanza&gt; praeferunt &lt;VERB&gt; gustandi ...</td>\n",
       "      <td>As if we were not far inferior in this even to...</td>\n",
       "      <td>la.en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;en&gt; &lt;no_stanza&gt; But let all things be done de...</td>\n",
       "      <td>omnia autem honeste et secundum ordinem fiant ...</td>\n",
       "      <td>en.la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;la&gt; Affert secundus ramus persicum, ex persic...</td>\n",
       "      <td>Affert secundus ramus persicum, ex persico, et...</td>\n",
       "      <td>la.la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33338</th>\n",
       "      <td>&lt;la&gt; &lt;no_stanza&gt; Sed vos religiosi, qui eam qu...</td>\n",
       "      <td>But it is you who are the really religious peo...</td>\n",
       "      <td>la.en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33339</th>\n",
       "      <td>&lt;la&gt; &lt;no_stanza&gt; ne forte decepti faciatis vob...</td>\n",
       "      <td>Lest ye corrupt yourselves, and make you a gra...</td>\n",
       "      <td>la.en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33340</th>\n",
       "      <td>&lt;en&gt; &lt;with_stanza&gt; And &lt;CCONJ&gt; thou &lt;PRON&gt; sha...</td>\n",
       "      <td>et diliges Dominum Deum tuum ex toto corde tuo...</td>\n",
       "      <td>en.la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33341</th>\n",
       "      <td>&lt;la&gt; Hanc fabulam ponit Lucanus: Fuit, inquit,...</td>\n",
       "      <td>Hanc fabulam ponit Lucanus: Fuit, inquit, in L...</td>\n",
       "      <td>la.la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33342</th>\n",
       "      <td>&lt;en&gt; &lt;no_stanza&gt; But thou shalt eat them befor...</td>\n",
       "      <td>Thraeicii quondam quam saeva licentia regis fe...</td>\n",
       "      <td>en.la</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33343 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  prompt  \\\n",
       "0      <la> <no_stanza> Aperuerat iam Italiam bellumq...   \n",
       "1      <la> <no_stanza> Usus practicus unus est ad al...   \n",
       "2      <la> <with_stanza> praeferunt <VERB> gustandi ...   \n",
       "3      <en> <no_stanza> But let all things be done de...   \n",
       "4      <la> Affert secundus ramus persicum, ex persic...   \n",
       "...                                                  ...   \n",
       "33338  <la> <no_stanza> Sed vos religiosi, qui eam qu...   \n",
       "33339  <la> <no_stanza> ne forte decepti faciatis vob...   \n",
       "33340  <en> <with_stanza> And <CCONJ> thou <PRON> sha...   \n",
       "33341  <la> Hanc fabulam ponit Lucanus: Fuit, inquit,...   \n",
       "33342  <en> <no_stanza> But thou shalt eat them befor...   \n",
       "\n",
       "                                                  answer prefix  \n",
       "0      The road into Italy had already been opened an...  la.en  \n",
       "1      One practical use is for altitude of geopotent...  la.en  \n",
       "2      As if we were not far inferior in this even to...  la.en  \n",
       "3      omnia autem honeste et secundum ordinem fiant ...  en.la  \n",
       "4      Affert secundus ramus persicum, ex persico, et...  la.la  \n",
       "...                                                  ...    ...  \n",
       "33338  But it is you who are the really religious peo...  la.en  \n",
       "33339  Lest ye corrupt yourselves, and make you a gra...  la.en  \n",
       "33340  et diliges Dominum Deum tuum ex toto corde tuo...  en.la  \n",
       "33341  Hanc fabulam ponit Lucanus: Fuit, inquit, in L...  la.la  \n",
       "33342  Thraeicii quondam quam saeva licentia regis fe...  en.la  \n",
       "\n",
       "[33343 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print samples of couples prompt / answer\n",
    "for i in range(5):\n",
    "    print(test_data.iloc[i]['prompt'])\n",
    "    print(test_data.iloc[i]['answer'])\n",
    "    print()\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;en&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;la&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;en.la&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;la.en&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;la.la&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;with_stanza&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;no_stanza&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;clue&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;EMPTY&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;ROOT&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;UNK&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SYM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            token\n",
       "0            <en>\n",
       "1            <la>\n",
       "2         <en.la>\n",
       "3         <la.en>\n",
       "4         <la.la>\n",
       "5   <with_stanza>\n",
       "6     <no_stanza>\n",
       "7          <clue>\n",
       "8         <EMPTY>\n",
       "9           <PAD>\n",
       "10         <ROOT>\n",
       "11          <UNK>\n",
       "12            ADJ\n",
       "13            ADP\n",
       "14            ADV\n",
       "15            AUX\n",
       "16          CCONJ\n",
       "17            DET\n",
       "18           INTJ\n",
       "19           NOUN\n",
       "20            NUM\n",
       "21           PART\n",
       "22           PRON\n",
       "23          PROPN\n",
       "24          PUNCT\n",
       "25          SCONJ\n",
       "26            SYM\n",
       "27           VERB\n",
       "28              X"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(path_to_special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mT5 model and tokenizer\n",
    "max_seq_len = 412                                   # Maximum number of tokens of the input sequence\n",
    "max_new_tokens = 412                                # Maximum number of tokens of the generated sequence\n",
    "model_name = \"/Data/AxelDlv/mt5-small-en-la-translation/mt5-small\"\n",
    "# checkpoint_path = f\"/Data/AxelDlv/mt5-small-en-la-translation/mt5-small-en-la-translation-final_no_stanza-last_epoch\"\n",
    "checkpoint_path = f\"/Data/AxelDlv/mt5-small-en-la-translation/mt5-small-en-la-translation-final_with_stanza-last_epoch\"\n",
    "STANZA = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MT5ForConditionalGeneration(\n",
       "      (shared): Embedding(250121, 512)\n",
       "      (encoder): MT5Stack(\n",
       "        (embed_tokens): Embedding(250121, 512)\n",
       "        (block): ModuleList(\n",
       "          (0): MT5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): MT5LayerSelfAttention(\n",
       "                (SelfAttention): MT5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 6)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): MT5LayerFF(\n",
       "                (DenseReluDense): MT5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-7): 7 x MT5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): MT5LayerSelfAttention(\n",
       "                (SelfAttention): MT5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): MT5LayerFF(\n",
       "                (DenseReluDense): MT5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): MT5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (decoder): MT5Stack(\n",
       "        (embed_tokens): Embedding(250121, 512)\n",
       "        (block): ModuleList(\n",
       "          (0): MT5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): MT5LayerSelfAttention(\n",
       "                (SelfAttention): MT5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 6)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): MT5LayerCrossAttention(\n",
       "                (EncDecAttention): MT5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): MT5LayerFF(\n",
       "                (DenseReluDense): MT5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-7): 7 x MT5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): MT5LayerSelfAttention(\n",
       "                (SelfAttention): MT5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): MT5LayerCrossAttention(\n",
       "                (EncDecAttention): MT5Attention(\n",
       "                  (q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                  (v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): MT5LayerFF(\n",
       "                (DenseReluDense): MT5DenseGatedActDense(\n",
       "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): MT5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=250121, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load special tokens and initialize tokenizer\n",
    "special_tokens = pd.read_csv(path_to_special_tokens)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens['token'].tolist()})\n",
    "\n",
    "# Load base model (without LoRA yet)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).cuda()\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define LoRA Configuration (MUST MATCH training config)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],  # LoRA layers added to 'q' and 'v' attention components\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "# Wrap base model with LoRA\n",
    "model = PeftModel(base_model, lora_config)\n",
    "model.load_state_dict(torch.load(f\"{checkpoint_path}.pt\", map_location=\"cuda\"))\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <la> Prima tabula est Baptismus, ubi deponitur vetus homo, et induitur novus; secunda, Poenitentia, qua post lapsum resurgimus, dum vetustas reversa repellitur, et novitas perdita resumitur. Post Baptismum prolapsi per Poenitentiam renovari valent, sed non per Baptismum. Licet homini saepius poenitere, sed non baptizari. Baptismus tantum est sacramentum; sed Poenitentia dicitur et sacramentum, et virtus mentis. Est enim Poenitentia interior, et est exterior. Exterior, sacramentum est; interior, virtus mentis est; et utraque causa salutis est et justificationis. Utrum vero omnis exterior poenitentia sit sacramentum, vel si non omnis, quae hoc nomine censenda sit, consequenter investigabimus. A poenitentia coepit Joannis praedicatio dicentis: #Poenitentiam agite; appropinquabit enim regnum coelorum.@# Quod autem praeco docuit, illud post Veritas praedicavit, exordium sumens sermonis a poenitentia. Poenitentia dicitur a puniendo, qua quis punit illicita quae commisit. Poenitentiae virtus timore concipitur. Unde Isaias: #A timore tuo, Domine, concepimus, et peperimus Spiritum salutis.@# Est autem poenitentia, ut ait Amb., mala praeterita plangere, et plangenda iterum non committere. Item Gregor.: Poenitere est anteacta peccata deflere, et flenda non committere. Nam qui sic alia deplorat, ut alia tamen committat, adhuc poenitentiam agere aut ignorat, aut dissimulat. Quid enim prodest si peccata luxuriae quis defleat, et adhuc avaritiae aestibus anhelat? His verbis quidam vehementius inhaerentes, contendunt vere poenitentem ultra non posse peccare damnabiliter; et si graviter peccaverit, veram non praecessisse poenitentiam. Quod etiam aliis muniunt testimoniis. Ait enim Isidor.: Irrisor est et non poenitens qui adhuc agit quod poenitet; nec videtur Deum poscere subditus, sed subsannare superbus. Canis reversus ad vomitum, et poenitens ad peccatum. Multi lacrymas indesinenter fundunt, et peccare non desinunt. Quosdam accipere lacrymas ad poenitentiam cerno, et affectum poenitentiae non habere; quia inconstantia mentis nunc recordatione peccati lacrymas fundunt, nunc reviviscente usu, ea quae fleverunt iterando. Isaias de peccatoribus dicit: #Lavamini, mundi estote.@# Lavatur et mundus est qui et praeterita plangit, et flenda iterum non committit. Lavatur, et non est mundus qui plangit quae gessit, nec deserit, et post lacrymas ea quae defleverat repetit. Item Aug.: Inanis est poenitentia, quam sequens culpa coinquinat. Nihil prosunt lamenta, si replicantur peccata. Nihil valet veniam a malis poscere, et mala denuo iterare. Item Greg.: Qui commissa plangit, nec tamen deserit, poenae graviori se subjicit. Item Amb.: Reperiuntur qui saepius agendam poenitentiam putant; qui luxuriantur in Christo: nam si vere poenitentiam in Christo agerent, iterandum postea non putarent; quia sicut unum Baptisma, ita est una Poenitentia. <la> <la.la> <la>\n",
      "Generated Text: Prima tabula est Baptismus, et induitur novus; secunda, Poenitentia est, quod non praecessisse poenitentiam. Quod etiam enim \n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: <en> <no_stanza> Yet she gives three hundred warrior hearts, for war a countless multitude. No javelin straps have they or swords flashing bale. On head and back they carry yellow lionskins, their national ornament, and pine staves arm them, arrows cram their inexhaustible quivers. <en> <en.la> <la> <no_stanza> \n",
      "Generated Text: Et a few hundreds of thousands, which they stood in ea, et triumphant in thee.\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: <la> perficiuntur; unde idem principium, medium et finem vocavit Orpheus. 6. Ad hoc sane provocandum atque tentandum te cogit necessitas per quam in discrimine positus homo appetit attrectatque pennas ut aiunt diluculi, ita volente Iove, qui ne humanum torpeat ingenium vividaque illius emoriatur ***, duris rebus urgentem addidit egestatem. 7. Memento Prometheum non placuisse Diis, utpote qui Deorum thesauros spargens, in torporem genus humanum concitare videretur, aut qui promiscue dignis et indignis quiddam excellentissimum commune faceret. 8. Habeas ergo tu tecumque pauci salutaris istius nectarei liquoris gustum, quo, cum Lethaei fluminis lethargicos humores penitus expurgaveris, facile primo coelestem cum Diis coelestibus vitam, mox supracoelestem cum supercoelestibus circuitum prosequare, unde cum copia attoniti vulgi iacentes Carneadem, Cineam et Metrodorum non altius intuebere consistentes. 9. Etsi tempus Pythagoricum indocilissimum omnium atque stultissimum quandoquidem per ipsum omnia oblivione deleantur comprobes, Simonidis quoque tempus cuius beneficio omnia quaeruntur, discuntur, inveniuntur, obliterata reclarescunt praecisaque repullulant non improbabis. 10. Cunctis, pro sufficientia, exquisitas alas tribuit natura, perpauci vero admodum sunt, qui eas norint explicare ad eum aerem dividendum atque pulsandum, qui non minus pulsandus ad volatum conducit atque praestat, quam dividendus obesse videtur; postquam enim ipsum laborans dividendo commoveris, ipse non ingratus te fulciendo promovebit. 11. Obiiciuntur nobis res, signa, imagines, spectra vel phantasmata. Quorum differentiae sunt odibile quod est deforme et malum, et appetibile quod est pulchrum et bonum, et composita ex duobus vel tribus horum, amabile bonum, odibile deforme; item captu facile, quod est nec excellens nec deficiens sensibile cum abstracto intelligibili, et captu difficile, quod est remissum vel excessivum sensibile et sua natura maxime cum sit sine abstractione intelligibile. 12. Quibus mediantibus natura ornat sensum, concupiscentiam, intellectum et voluntatem, unde prodit videre, tangere in genere, imaginari, cogitare, primam memorari, ratiocinari et intelligere, unde nascitur secundum memoratum, quod intellectus adeptus et in habitu consuevit appellari. Quibus adiicitur formatio, cuius species sunt: opinio in genere, aporia, scrupulus, syndaeresis, fiducia et illectio in genere, quae excitat voluptatem, ambitionem, curiositatem et fiduciam, et profligatio quaedam incitans in abominationem, terrorem et horrorem. Quae omnia pariunt hinc electionem et fugam, inde assensum et dissensionem. 13. In iis est simplex apprehensio vel conceptus primus, numeratio, mensuratio, ponderatio, divisio, distributio, distinctio, ordinatio, definitio, propositio, argumentatio, intellectio, quae est simplex conceptus secundus et quam ita placet appellare mentatio. 14. <la> <la.la> <la>\n",
      "Generated Text: perficiuntur; unde idem principium, qui non minus pulsandus ad volatum conducit atque praestat, ipse non ingratus te fulciendo promovebit. \n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: <la> <no_stanza> demonstravit hanc istius  consuetudinem ac rationem fuisse: quod omnis frumenti copia decumarum nomine penes istum esset redacta, solitum esse  istum pecuniam cogere a civitatibus, frumentum improbare, quantum frumenti esset Romam mittendum, tantum de suo quaestu ac de sua copia  frumenti mittere. posco rationes, inspicio litteras, video frumenti granum  Halaesinos, quibus sexagena milia modium imperata erant, nullum dedisse, pecuniam Volcatio,  Timarchidi, scribae   dedisse: reperio genus huius modi, iudices, praedae, ut praetor, qui frumentum emere  debeat , non emat sed vendat, pecunias, quas civitatibus distribuere  debeat , eas omnis avertat atque auferat. non mihi iam furtum, sed  monstrum ac prodigium videbatur civitatum frumentum improbare, suum probare ; cum suum  probasset , pretium ei frumento constituere; quod constituisset, id a civitatibus  auferre, quod a populo Romano accepisset, tenere. <la> <la.en> <en> <no_stanza> \n",
      "Generated Text: But when they shall be suitable for the king of the city, which would not become a prisoner of their tribe,\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: <la> <with_stanza> et <CCONJ> si <SCONJ> quidem <PART> fecerit <VERB> fructum <NOUN> sin <PART> autem <PART> in <ADP> futurum <ADJ> succides <VERB> eam <PRON> <la> <la.en> <en> <with_stanza> \n",
      "Generated Text: Then < > the < > king < shall < > be <aux > in < > his <\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: <la> <no_stanza> et non parcet oculus meus neque miserebor sed vias tuas inponam tibi et abominationes tuae in medio tui erunt et scietis quia ego sum Dominus percutiens <la> <la.en> <en> <no_stanza> \n",
      "Generated Text: But when they shall be satisfied with a few hundred thousand of people, which would have been given to the people of the nations.\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: <la> <no_stanza> tandem prorumpere adactus Oeclides: alio curarum agitante tumultu non equidem effreno iuvenis clamore profani dictorumque metu, licet hic insana minetur, elicior tenebris; alio mihi debita fato summa dies, vetitumque dari mortalibus armis. <la> <la.en> <en> <no_stanza> \n",
      "Generated Text: But when they shall be suitable for a few hundred thousand of people, which would be the king of the people of their children.\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: <la> <no_stanza> Nihil est nobis dictu, visu, auditu cum insania circi, cum inpudicitia theatri, cum atrocitate arenae, cum xysti vanitate. <la> <la.en> <en> <no_stanza> \n",
      "Generated Text: But when they shall be satisfied with a few hundred of thousand of people, which would be the king of the people.\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: <la> Aves quando volant ad aethera, formam crucis sumunt; homo natans per aquas, forma crucis vehitur; navis per maria, antenna cruci similata sufflatur; tau littera, signum salutis et crucis describitur. Beda. Vel in transverso ligno crucis, ubi figuntur manus, gaudium spei signatur; per manus enim opera, per latitudinem hilaritatem operantis intelligimus; quia tristitia facit angustias; per altitudinem, cui caput adiungitur, expectationem retributionis de sublimi iustitia Dei; per longitudinem, qua totum corpus extenditur, tolerantiam, unde longanimes dicuntur; per profundum quod terrae est infixum, ipsum sacramentum secretum. Quamdiu ergo hic agunt corpora nostra ut destruatur corpus peccati, tempus nobis est crucis. Theophylactus. Quod autem ponebant sortem super vestimenta eius, et hoc illudentes fecerunt, quasi regis vestimenta dividentes: etenim vilia erant, non valde pretiosa. Hoc autem Ioannes Evangelista plenius exponit, quia scilicet milites cetera in quatuor partes iuxta suum numerum dividentes, de tunica, quae inconsutilis erat, desuper contexta per totum, sortem miserunt. Hieronymus. Vestimenta autem domini, eius mandata sunt, quibus tegitur corpus eius, idest Ecclesia: quae dividunt inter se milites gentium, ut sint quatuor ordines cum una fide, idest coniugati et viduati, praepositi et separati. Sortiti sunt tunicam indivisam, quae est pax et unitas. Sequitur erat autem hora tertia, et crucifixerunt eum. Hoc vere et proprie Marcus intulit: nam sexta hora tenebrae suffuderunt terram, ut non quisquam potuisset movere caput. Augustinus de Cons. Evang. Si hora quasi sexta Pilato sedente pro tribunali traditus est Iesus crucifigendus Iudaeis, ut Ioannes refert; quomodo hora tertia crucifixus est, sicut verba Marci non intelligentes quidam putaverunt? Prius ergo qua hora crucifigi potuerit videamus; deinde videbimus cur hora tertia crucifixum dixerit Marcus. Hora erat quasi sexta cum traditus est crucifigendus a Pilato sedente pro tribunali, ut dictum est: non enim iam plena sexta erat, sed quasi sexta, idest peracta quinta, et aliquid etiam de sexta esse coeperat, ut peracta quinta et inchoata sexta, gererentur haec quae narrata sunt in crucifixione domini nostri, donec completa sexta, illo pendente fierent hae quae dicuntur tenebrae. Quaeramus autem iam cur dixerit Marcus erat autem hora tertia, et crucifixerunt eum. Iam certe dixerat et crucifigentes eum, diviserunt vestimenta eius; sic etiam ceteri attestantur, quod eo crucifixo vestimenta divisa sunt. Si enim rei gestae tempus voluit commemorare Marcus, sufficeret dicere: erat autem hora tertia: ut quid adiunxit et crucifixerunt eum? <la> <la.la> <la>\n",
      "Generated Text: Aves quando volant ad aethera, formam crucis eius, cui caput adiungitur, quomodo hora tertia crucifixus \n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: <la> <with_stanza> nunc <ADV> en <VERB> , <PUNCT> nunc <ADV> , <PUNCT> inquam <VERB> falsa <ADJ> ut <SCONJ> praesagia <NOUN> nostra <DET> sint <AUX> , <PUNCT> oro <VERB> , <PUNCT> mensque <NOUN> augurio <NOUN> ludatur <VERB> inani <ADJ> haud <SCONJ> procul <NOUN> est <AUX> funesta <ADJ> dies <NOUN> , <PUNCT> atrocia <NOUN> novi <NOUN> corda <NOUN> ac <CCONJ> prospicio <VERB> natas <VERB> e <ADP> cladibus <NOUN> iras <NOUN> . <PUNCT> <la> <la.en> <en> <with_stanza> \n",
      "Generated Text: Then < > the < > king < shall < > be <aux > in < > his <\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_data_ = test_data.sample(10)\n",
    "\n",
    "outputs = inference_from_csv(model, test_data_, tokenizer, batch_size=8, max_seq_len=512, column_prompt=\"prompt\", use_amp=True)\n",
    "for item in outputs:\n",
    "    print(\"Prompt:\", item[\"prompt\"])\n",
    "    print(\"Generated Text:\", item[\"generated_text\"])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>prefix</th>\n",
       "      <th>generated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lotis prius intestinis et pedibus totumque si...</td>\n",
       "      <td>Victory for Thebes is certain. Fear not. Neith...</td>\n",
       "      <td>la.en</td>\n",
       "      <td>But when they shall be satisfied with their fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nec parasitorum nobis assentatio in comoediis...</td>\n",
       "      <td>Nec parasitorum nobis assentatio in comoediis ...</td>\n",
       "      <td>la.la</td>\n",
       "      <td>Nec parasitorum nobis assentatio in comoediis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoc igitur primitus cognito, quod ea ipsa qua...</td>\n",
       "      <td>Hoc igitur primitus cognito, quod ea ipsa quae...</td>\n",
       "      <td>la.la</td>\n",
       "      <td>Hoc igitur primitus cognito, quod ea ipsa quae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ergo raptorem filiae meae, violatorem foederi...</td>\n",
       "      <td>Seventy-two thousand oxen,</td>\n",
       "      <td>la.en</td>\n",
       "      <td>And when they were a king of the tribe of thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dum patimur, leguntur; dum recognoscimus, pro...</td>\n",
       "      <td>While we suffer, it is all read in the book; t...</td>\n",
       "      <td>la.en</td>\n",
       "      <td>But when they shall be satisfied, which would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>Datum V Idus Julias per manum Sergii biblioth...</td>\n",
       "      <td>Datum V Idus Julias per manum Sergii bibliothe...</td>\n",
       "      <td>la.la</td>\n",
       "      <td>Datum V Idus Julias per manum Sergii bibliothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>egressus ad eos Loth post tergum adcludens os...</td>\n",
       "      <td>These are the men who spin out law cases when ...</td>\n",
       "      <td>la.en</td>\n",
       "      <td>And when they went out of the town, and took o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Homo similiter Deus esse voluit, cui persuasu...</td>\n",
       "      <td>Homo similiter Deus esse voluit, cui persuasum...</td>\n",
       "      <td>la.la</td>\n",
       "      <td>Homo similiter Deus esse voluit, cui persuasum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Adstitit enim mihi quidam candido praeclarus ...</td>\n",
       "      <td>When this was done, and they had all made a co...</td>\n",
       "      <td>la.en</td>\n",
       "      <td>But when they would be suitable to us, which s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>Memorat veterem ejus amicitiam cum beatae mem...</td>\n",
       "      <td>Memorat veterem ejus amicitiam cum beatae memo...</td>\n",
       "      <td>la.la</td>\n",
       "      <td>Memorat veterem ejus amicitiam cum beatae memo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt  \\\n",
       "0      lotis prius intestinis et pedibus totumque si...   \n",
       "1      Nec parasitorum nobis assentatio in comoediis...   \n",
       "2      Hoc igitur primitus cognito, quod ea ipsa qua...   \n",
       "3      Ergo raptorem filiae meae, violatorem foederi...   \n",
       "4      Dum patimur, leguntur; dum recognoscimus, pro...   \n",
       "...                                                 ...   \n",
       "1995   Datum V Idus Julias per manum Sergii biblioth...   \n",
       "1996   egressus ad eos Loth post tergum adcludens os...   \n",
       "1997   Homo similiter Deus esse voluit, cui persuasu...   \n",
       "1998   Adstitit enim mihi quidam candido praeclarus ...   \n",
       "1999   Memorat veterem ejus amicitiam cum beatae mem...   \n",
       "\n",
       "                                                 answer prefix  \\\n",
       "0     Victory for Thebes is certain. Fear not. Neith...  la.en   \n",
       "1     Nec parasitorum nobis assentatio in comoediis ...  la.la   \n",
       "2     Hoc igitur primitus cognito, quod ea ipsa quae...  la.la   \n",
       "3                           Seventy-two thousand oxen,   la.en   \n",
       "4     While we suffer, it is all read in the book; t...  la.en   \n",
       "...                                                 ...    ...   \n",
       "1995  Datum V Idus Julias per manum Sergii bibliothe...  la.la   \n",
       "1996  These are the men who spin out law cases when ...  la.en   \n",
       "1997  Homo similiter Deus esse voluit, cui persuasum...  la.la   \n",
       "1998  When this was done, and they had all made a co...  la.en   \n",
       "1999  Memorat veterem ejus amicitiam cum beatae memo...  la.la   \n",
       "\n",
       "                                         generated_text  \n",
       "0     But when they shall be satisfied with their fe...  \n",
       "1     Nec parasitorum nobis assentatio in comoediis ...  \n",
       "2     Hoc igitur primitus cognito, quod ea ipsa quae...  \n",
       "3     And when they were a king of the tribe of thei...  \n",
       "4     But when they shall be satisfied, which would ...  \n",
       "...                                                 ...  \n",
       "1995  Datum V Idus Julias per manum Sergii bibliothe...  \n",
       "1996  And when they went out of the town, and took o...  \n",
       "1997  Homo similiter Deus esse voluit, cui persuasum...  \n",
       "1998  But when they would be suitable to us, which s...  \n",
       "1999  Memorat veterem ejus amicitiam cum beatae memo...  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove every special token from the tokenizer ie every <...>, using regex\n",
    "pattern = r\"<.*?>\"\n",
    "test_data_ = test_data.sample(2000).reset_index(drop=True)\n",
    "test_data_ = inference_from_csv_adding_column(model, \n",
    "                                              test_data_, \n",
    "                                              tokenizer, \n",
    "                                              batch_size=8, \n",
    "                                              max_seq_len=max_new_tokens, \n",
    "                                              column_prompt=\"prompt\",\n",
    "                                              new_column=\"generated_text\", \n",
    "                                              use_amp=True)\n",
    "test_data_ = test_data_.replace(to_replace=pattern, value=\"\", regex=True)\n",
    "test_data_ = test_data_.replace(to_replace=r\"\\n\", value=\" \", regex=True)\n",
    "test_data_ = test_data_.replace(to_replace=r\"\\s+\", value=\" \", regex=True) # remove multiple spaces\n",
    "test_data_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score, chrf_score = calculate_bleu_and_chrf(\n",
    "    test_data_, \"en\", \"la\", tokenizer, model,\n",
    "    max_examples_to_test=500,\n",
    "    column_prompt=\"prompt\", column_target=\"answer\", column_prefix=\"prefix\",\n",
    "    max_input_len=max_seq_len, max_output_len=max_new_tokens\n",
    ")\n",
    "print(f\"BLEU Score: {bleu_score}, CHRF Score: {chrf_score}\")\n",
    "\n",
    "bleu_score, chrf_score = calculate_bleu_and_chrf(\n",
    "    test_data_, \"la\", \"en\", tokenizer, model,\n",
    "    max_examples_to_test=500,\n",
    "    column_prompt=\"prompt\", column_target=\"answer\", column_prefix=\"prefix\",\n",
    "    max_input_len=max_seq_len, max_output_len=max_new_tokens\n",
    ")\n",
    "print(f\"BLEU Score: {bleu_score}, CHRF Score: {chrf_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on summaries with mistral 'en' -> 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.summary_mistral import load_model, mistral_summarize_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters for Mistral summarization\n",
    "MODEL_PATH = \"/Data/AxelDlv/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer_name = \"tokenizer.model.v3\"\n",
    "DOWNLOAD_MODEL = False\n",
    "\n",
    "instruction = (\n",
    "    \"Summarize the following text in a clear and concise manner while preserving its core meaning and key details. \"\n",
    "    \"Focus on capturing the main ideas, avoiding unnecessary repetitions, and ensuring coherence. \"\n",
    "    \"Keep the summary informative and fluent while maintaining the original context.\"\n",
    ")\n",
    "max_new_tokens = 1000\n",
    "temperature = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Mistral model and tokenizer\n",
    "model_mistral, tokenizer_mistral = load_model(MODEL_PATH, tokenizer_name, DOWNLOAD_MODEL)\n",
    "model_mistral.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_summarize = \"The quick brown fox jumps over the lazy dog.\"\n",
    "summary = mistral_summarize_texts(\n",
    "    text_to_summarize,\n",
    "    model_mistral,\n",
    "    tokenizer_mistral,\n",
    "    max_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    instruction=instruction\n",
    ")\n",
    "\n",
    "print(f\"Input: {text_to_summarize}\")\n",
    "print(f\"Summary: {summary}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LatinSummarizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
